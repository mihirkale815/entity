{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "torch.manual_seed(1)\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from data.utils import DatasetUtil\n",
    "from torchtext.data import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"../data/mit-movie-1/original/\"\n",
    "TRAIN_FILE = \"engtrain.bio\"\n",
    "TEST_FILE = \"engtest.bio\"\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['cuda'] = False\n",
    "args['datapath'] = DATAPATH\n",
    "args['filename'] = TRAIN_FILE\n",
    "args['batch_size'] = TRAIN_BATCH_SIZE\n",
    "datasetutil = DatasetUtil(args)\n",
    "train_iter = datasetutil.get_train_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['filename'] = TEST_FILE\n",
    "args['batch_size'] = TEST_BATCH_SIZE\n",
    "test_iter = datasetutil.get_iterator(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in test_iter:pass\n",
    "test_stoi = batch.dataset.fields['word'].vocab.stoi\n",
    "\n",
    "for batch in train_iter:pass\n",
    "train_stoi = batch.dataset.fields['word'].vocab.stoi\n",
    "\n",
    "train_stoi == test_stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, bidirectional=True):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.ix_to_tag = {v:k for k,v in self.tag_to_ix.items()}\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=bidirectional)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.loss_function = nn.NLLLoss()\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        return (autograd.Variable(torch.randn(2, batch_size, self.hidden_dim // 2)),\n",
    "                autograd.Variable(torch.randn(2, batch_size, self.hidden_dim // 2)))\n",
    "\n",
    "\n",
    "    def _get_lstm_features(self, batch):\n",
    "        batch_size = batch.size()[1]\n",
    "        #self.hidden = self.init_hidden(batch_size)\n",
    "        embeds = self.word_embeds(batch)\n",
    "        lstm_out, self.hidden = self.lstm(embeds)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def loss(self,batch,tags):\n",
    "        score, tag_seq = self.forward(batch)\n",
    "        total_loss = self.loss_function(score, tags.view(batch_size * seq_len))\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def forward(self, batch):  \n",
    "        batch_size = batch.size()[1]\n",
    "        seq_len = batch.size()[0]\n",
    "        lstm_feats = self._get_lstm_features(batch)\n",
    "        lstm_feats = lstm_feats.view(batch_size * seq_len, -1)\n",
    "        score = F.log_softmax(lstm_feats, 1)\n",
    "        _, tag_seq  = torch.max(lstm_feats, dim=1)\n",
    "        tag_seq = tag_seq.view(batch_size, seq_len)\n",
    "        return score, tag_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 14\n",
    "model = BiLSTM(len(datasetutil.WORD.vocab.stoi),datasetutil.TAG.vocab.stoi, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0\n",
      "Variable containing:\n",
      "1.00000e+05 *\n",
      "  1.3912\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  1\n",
      "Variable containing:\n",
      " 92496.3125\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  2\n",
      "Variable containing:\n",
      " 75364.6719\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  3\n",
      "Variable containing:\n",
      " 66269.1797\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  4\n",
      "Variable containing:\n",
      " 60401.6328\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  5\n",
      "Variable containing:\n",
      " 56254.1680\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  6\n",
      "Variable containing:\n",
      " 53093.1641\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  7\n",
      "Variable containing:\n",
      " 50656.6719\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  8\n",
      "Variable containing:\n",
      " 48493.1289\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  9\n",
      "Variable containing:\n",
      " 46665.6523\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  10\n",
      "Variable containing:\n",
      " 45029.0039\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  11\n",
      "Variable containing:\n",
      " 43725.8164\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  12\n",
      "Variable containing:\n",
      " 42479.7461\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  13\n",
      "Variable containing:\n",
      " 41207.4219\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  14\n",
      "Variable containing:\n",
      " 40234.0391\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  15\n",
      "Variable containing:\n",
      " 39275.1641\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  16\n",
      "Variable containing:\n",
      " 38404.1328\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  17\n",
      "Variable containing:\n",
      " 37677.8164\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  18\n",
      "Variable containing:\n",
      " 36956.8477\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  19\n",
      "Variable containing:\n",
      " 36319.3945\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  20\n",
      "Variable containing:\n",
      " 35813.2617\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  21\n",
      "Variable containing:\n",
      " 35193.6836\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  22\n",
      "Variable containing:\n",
      " 34528.9297\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  23\n",
      "Variable containing:\n",
      " 34139.9883\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  24\n",
      "Variable containing:\n",
      " 33632.9102\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  25\n",
      "Variable containing:\n",
      " 33210.5195\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  26\n",
      "Variable containing:\n",
      " 32507.4297\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  27\n",
      "Variable containing:\n",
      " 32361.2207\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  28\n",
      "Variable containing:\n",
      " 31791.0605\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  29\n",
      "Variable containing:\n",
      " 31464.0176\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  30\n",
      "Variable containing:\n",
      " 30887.5684\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  31\n",
      "Variable containing:\n",
      " 30482.3379\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  32\n",
      "Variable containing:\n",
      " 30462.1445\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  33\n",
      "Variable containing:\n",
      " 29856.0625\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  34\n",
      "Variable containing:\n",
      " 29601.6660\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  35\n",
      "Variable containing:\n",
      " 29297.2578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  36\n",
      "Variable containing:\n",
      " 29354.7793\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  37\n",
      "Variable containing:\n",
      " 28822.2773\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  38\n",
      "Variable containing:\n",
      " 28507.9883\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  39\n",
      "Variable containing:\n",
      " 28171.0215\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  40\n",
      "Variable containing:\n",
      " 27938.3711\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  41\n",
      "Variable containing:\n",
      " 27930.1250\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  42\n",
      "Variable containing:\n",
      " 28127.1641\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  43\n",
      "Variable containing:\n",
      " 27547.5996\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch =  44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3375cfd81ff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/entity/venv/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/entity/venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(\n",
    "        100):  \n",
    "    print(\"epoch = \",epoch)\n",
    "    epoch_loss = 0\n",
    "    for idx ,batch in enumerate(train_iter):\n",
    "\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        loss = model.loss(batch.word, batch.tag)\n",
    "        epoch_loss+=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_output(path,Actual,Preds,Data):\n",
    "    fout = open(path,\"w\")\n",
    "    for i in range(0,len(Preds)):\n",
    "        sentence = Data[i]\n",
    "        predicted = Preds[i]\n",
    "        actual = Actual[i]\n",
    "        for idx in range(0,len(actual)):\n",
    "            fout.write(\" \".join([sentence[idx],'UNK','UNK',actual[idx],predicted[idx]]) + \"\\n\")\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(model,dataset_iter):\n",
    "    Actual = []\n",
    "    Preds = []\n",
    "    Sentences = []\n",
    "    for batch in dataset_iter:\n",
    "        score,tags = model(batch.word)\n",
    "        for i in range(tags.size()[0]):\n",
    "            sentence = batch.word[:,i]\n",
    "            actual_tags = batch.tag[:,i]\n",
    "            predicted_tags = tags[i,:]\n",
    "            sentence = [datasetutil.WORD.vocab.itos[idx.data[0]] for idx in sentence]\n",
    "            predicted = [datasetutil.TAG.vocab.itos[idx.data[0]] for idx in predicted_tags]\n",
    "            actual = [datasetutil.TAG.vocab.itos[idx.data[0]] for idx in actual_tags]\n",
    "            Actual.append(actual)\n",
    "            Preds.append(predicted)\n",
    "            Sentences.append(sentence)\n",
    "    return Actual,Preds,Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actual,Preds,Sentences = get_output(model,test_iter)\n",
    "test_output_path = \"test.res\"\n",
    "persist_output(test_output_path,Actual,Preds,Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actual,Preds,Sentences = get_output(model,train_iter)\n",
    "train_output_path = \"train.res\"\n",
    "persist_output(train_output_path,Actual,Preds,Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
